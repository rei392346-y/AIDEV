{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xu0GaP80H0M"
      },
      "source": [
        "# 課題5：映画レビューの評判分析\n",
        "\n",
        "本課題ではAmazon傘下の「IMDb」に投稿された映画のレビュー（英語）を分析し、レビューがPositive（ポジティブ）か、Negative（ネガティブ）かの判別を行ないます。\n",
        "\n",
        "データセットは、以下のサイトで配布されているものを利用します。\n",
        "\n",
        "[Large Movie Review Dataset](https://ai.stanford.edu/%7Eamaas/data/sentiment/)\n",
        "\n",
        "わからない場合は、ここまでのレッスン内容や各種ライブラリの公式ドキュメントを参照しましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnNp3e4X1VzH"
      },
      "source": [
        "## 1. 必要なライブラリのimport"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzALkNahz3So"
      },
      "outputs": [],
      "source": [
        "# （変更しないでください）\n",
        "\n",
        "# 必要なライブラリのimport\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# 文章ファイル検索用\n",
        "import glob\n",
        "import collections\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "\n",
        "# DataFrameですべての列を表示する設定\n",
        "pd.options.display.max_columns = None\n",
        "\n",
        "# seabornによる装飾を適用する\n",
        "sns.set_theme()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "設定したRANDOM_STATE= 42\n"
          ]
        }
      ],
      "source": [
        "# ===== 日本語フォント設定（Mac向け／文字化け対策）=====\n",
        "from matplotlib import font_manager, rcParams\n",
        "_jp_candidates = [\"Hiragino Sans\", \"Hiragino Kaku Gothic ProN\", \"Hiragino Maru Gothic ProN\", \"IPAexGothic\", \"Noto Sans CJK JP\"]\n",
        "_available = {f.name for f in font_manager.fontManager.ttflist}\n",
        "for _f in _jp_candidates:\n",
        "    if _f in _available:\n",
        "        rcParams[\"font.family\"] = _f\n",
        "        break\n",
        "rcParams[\"axes.unicode_minus\"] = False\n",
        "\n",
        "# ======== 乱数シード設定（要件：random_state を外す/可変化）========\n",
        "# None にすると固定しません。random_stateを固定化するときは 0とか42とか などの整数を入れる\n",
        "# RANDOM_STATE = None  # デバッグ時はコメントアウトする\n",
        "RANDOM_STATE = 42 # デバッグ時指定用\n",
        "print(\"設定したRANDOM_STATE=\",RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3pjMa-V1j3i"
      },
      "source": [
        "## 2. データの読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7uOIUKE46-c"
      },
      "outputs": [],
      "source": [
        "%%capture \n",
        "# ↑ログが５万行でるので、出力を抑止\n",
        "# ダウンロードした圧縮ファイルを解凍する（変更しないでください）\n",
        "!tar zxvf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiKcOmlbJQ1r"
      },
      "source": [
        "*./aclImdb* フォルダ内にあるファイルを読み込みます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSQxdWMe5Dpm"
      },
      "outputs": [],
      "source": [
        "# trainフォルダのファイル一覧を取得（変更しないでください）\n",
        "train_neg_files = glob.glob(\"./aclImdb/train/neg/*\")\n",
        "train_pos_files = glob.glob(\"./aclImdb/train/pos/*\")\n",
        "\n",
        "# testフォルダのファイル一覧を取得（変更しないでください）\n",
        "test_neg_files = glob.glob(\"./aclImdb/test/neg/*\")\n",
        "test_pos_files = glob.glob(\"./aclImdb/test/pos/*\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_neg_files[3]= ./aclImdb/train/neg/9056_1.txt\n",
            "train_neg_files (first 5)= ['./aclImdb/train/neg/1821_4.txt', './aclImdb/train/neg/10402_1.txt', './aclImdb/train/neg/1062_4.txt', './aclImdb/train/neg/9056_1.txt', './aclImdb/train/neg/5392_3.txt']\n",
            "train_pos_files (first 5)= ['./aclImdb/train/pos/4715_9.txt', './aclImdb/train/pos/12390_8.txt', './aclImdb/train/pos/8329_7.txt', './aclImdb/train/pos/9063_8.txt', './aclImdb/train/pos/3092_10.txt']\n",
            "test_neg_files (first 5)= ['./aclImdb/test/neg/1821_4.txt', './aclImdb/test/neg/9487_1.txt', './aclImdb/test/neg/4604_4.txt', './aclImdb/test/neg/2828_2.txt', './aclImdb/test/neg/10890_1.txt']\n",
            "test_pos_files (first 5)= ['./aclImdb/test/pos/4715_9.txt', './aclImdb/test/pos/1930_9.txt', './aclImdb/test/pos/3205_9.txt', './aclImdb/test/pos/10186_10.txt', './aclImdb/test/pos/147_10.txt']\n"
          ]
        }
      ],
      "source": [
        "# 中身を確認\n",
        "print(\"train_neg_files[3]=\" , train_neg_files[3] )\n",
        "\n",
        "print(\"train_neg_files (first 5)=\" , train_neg_files[:5])\n",
        "print(\"train_pos_files (first 5)=\" ,train_pos_files[:5])\n",
        "print(\"test_neg_files (first 5)=\" , test_neg_files[:5])\n",
        "print(\"test_pos_files (first 5)=\" , test_pos_files[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwILbT3E3uRu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train neg: 12500\n",
            "train pos: 12500\n",
            "test  neg: 12500\n",
            "test  pos: 12500\n"
          ]
        }
      ],
      "source": [
        "# それぞれのファイル数を確認\n",
        "print(\"train neg:\", len(train_neg_files))\n",
        "print(\"train pos:\", len(train_pos_files))\n",
        "print(\"test  neg:\", len(test_neg_files))\n",
        "print(\"test  pos:\", len(test_pos_files))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOezW34ZJfwA"
      },
      "source": [
        "前処理をするため、合計50000あるファイルをリストにまとめます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PJps6pk5xP1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ファイル名をまとめたリストを用意（変更しないでください）\n",
        "filenames = train_neg_files + train_pos_files + test_neg_files + test_pos_files\n",
        "\n",
        "# filenamesの長さを確認（変更しないでください）\n",
        "len(filenames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "filenames[0]= ./aclImdb/train/neg/1821_4.txt\n",
            "filenames[-1]= ./aclImdb/test/pos/1917_10.txt\n"
          ]
        }
      ],
      "source": [
        "# filenamesの中身を確認\n",
        "print(\"filenames[0]=\" , filenames[0])\n",
        "print(\"filenames[-1]=\" , filenames[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOVm-xBvJuQd"
      },
      "source": [
        "リストの最初と最後のファイルを確認してみます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D10DLJRv5y2d"
      },
      "outputs": [],
      "source": [
        "# エンコーディング用定数（変更しないでください）\n",
        "ENCODING = 'utf-8'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tR1blqz-6ytX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(first_text)= 250\n",
            "first_text= Working with one of the best Shakespeare sources, this film manages to be creditable to it's source, whilst still appealing to a wider audience.<br /><br />Branagh steals the film from under Fishburne's nose, and there's a talented cast on good form.\n"
          ]
        }
      ],
      "source": [
        "# 最初のファイルの内容を確認\n",
        "with open(filenames[0], \"r\", encoding=ENCODING, errors=\"ignore\") as f:\n",
        "    first_text = f.read()\n",
        "\n",
        "print(\"len(first_text)=\" ,len(first_text))\n",
        "print(\"first_text=\",first_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrvFZOcc62RQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(last_text)= 553\n",
            "last_text= I saw this movie on TV and loved it! I am a real disaster film fan, and this one was great. The cast was made of some really interesting people. Connie Selleca is always great. And William Devane is in a league of his own. He can play both comedy and thriller in the same movie like few others can. The story line is great too. The thought of being able to follow a time line of what will happen, and to use this time line to prevent a global disaster is an interesting idea. And this movie brings it out in such a way that is almost totally believable.\n"
          ]
        }
      ],
      "source": [
        "# 最後のファイルの内容を確認\n",
        "with open(filenames[-1], \"r\", encoding=ENCODING, errors=\"ignore\") as f:\n",
        "    last_text = f.read()\n",
        "\n",
        "print(\"len(last_text)=\" ,len(last_text))\n",
        "print(\"last_text=\",last_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1MSLkqD9o_i"
      },
      "source": [
        "## 3. データの前処理\n",
        "\n",
        "データの前処理として、形態素解析と行列への変換を行ないます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iir0OFu_Xu1"
      },
      "source": [
        "### 形態素解析"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wai2oEG39jF8"
      },
      "outputs": [],
      "source": [
        "# 文字列の中で使われている単語ごとの数を返す関数を作成\n",
        "#（レッスン本編の内容を確認して、下記にコードを追記してください）\n",
        "# 文字列のなかで使われている単語ごとの数を返す\n",
        "def get_word_count(text, min_length=3):\n",
        "    # ノイズの除去：不要と思われる文字を除去する\n",
        "    for ch in \".,:;!?-+*/=()[]{}<>~^#$@%&'\\\"_0123456789\":\n",
        "        text = text.replace(ch, ' ')\n",
        "\n",
        "    # 形態素解析：文章を単語に分割\n",
        "    _words = text.strip().split()\n",
        "\n",
        "    # 表記のゆれの補正：\n",
        "    # 単語のリストを受け取り、指定された文字数以上の単語だけをすべて小文字にして返す\n",
        "    _words = [_word.lower() for _word in _words if len(_word) >= min_length]\n",
        "\n",
        "    # collections.Counterの戻り値は辞書型のサブクラス\n",
        "    _count = collections.Counter(_words)\n",
        "\n",
        "    # 辞書型に変換して返す\n",
        "    return dict(_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BmIbEf699gs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('working', 1), ('with', 1), ('one', 1), ('the', 2), ('best', 1), ('shakespeare', 1), ('sources', 1), ('this', 1), ('film', 2), ('manages', 1)]\n"
          ]
        }
      ],
      "source": [
        "# 最初のファイルを使って、先ほど作成した関数をテスト\n",
        "print(list(get_word_count(first_text).items())[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sze6ofim-OTR"
      },
      "outputs": [],
      "source": [
        "# 単語ごとの数のリストを作成（変更しないでください）\n",
        "word_count_data = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bN54gjEh-QKj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processed: 5000/50000\n",
            "processed: 10000/50000\n",
            "processed: 15000/50000\n",
            "processed: 20000/50000\n",
            "processed: 25000/50000\n",
            "processed: 30000/50000\n",
            "processed: 35000/50000\n",
            "processed: 40000/50000\n",
            "processed: 45000/50000\n",
            "processed: 50000/50000\n"
          ]
        }
      ],
      "source": [
        "# すべてのファイルに対して、先ほど作成した関数を実行\n",
        "# --- 再実行時の重複を防止 ---\n",
        "word_count_data.clear()\n",
        "\n",
        "for i, fn in enumerate(filenames, start=1):\n",
        "    with open(fn, \"r\", encoding=ENCODING, errors=\"ignore\") as f:\n",
        "        text = f.read()\n",
        "        counts = get_word_count(text)\n",
        "        word_count_data.append(counts)\n",
        "\n",
        "    # 進捗表示（重ければコメントアウト可）\n",
        "    if i % 5000 == 0:\n",
        "        print(f\"processed: {i}/{len(filenames)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIIkPnNL-gQP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(word_count_data): 50000\n"
          ]
        }
      ],
      "source": [
        "# 単語ごとの数のリストの長さを確認\n",
        "print(\"len(word_count_data):\", len(word_count_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word_count_data[0] {'working': 1, 'with': 1, 'one': 1, 'the': 2, 'best': 1, 'shakespeare': 1, 'sources': 1, 'this': 1, 'film': 2, 'manages': 1, 'creditable': 1, 'source': 1, 'whilst': 1, 'still': 1, 'appealing': 1, 'wider': 1, 'audience': 1, 'branagh': 1, 'steals': 1, 'from': 1, 'under': 1, 'fishburne': 1, 'nose': 1, 'and': 1, 'there': 1, 'talented': 1, 'cast': 1, 'good': 1, 'form': 1}\n"
          ]
        }
      ],
      "source": [
        "# 中身を確認\n",
        "print(\"word_count_data[0]\", word_count_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZGy0O5F-gje"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'working': 1,\n",
              " 'with': 1,\n",
              " 'one': 1,\n",
              " 'the': 2,\n",
              " 'best': 1,\n",
              " 'shakespeare': 1,\n",
              " 'sources': 1,\n",
              " 'this': 1,\n",
              " 'film': 2,\n",
              " 'manages': 1,\n",
              " 'creditable': 1,\n",
              " 'source': 1,\n",
              " 'whilst': 1,\n",
              " 'still': 1,\n",
              " 'appealing': 1,\n",
              " 'wider': 1,\n",
              " 'audience': 1,\n",
              " 'branagh': 1,\n",
              " 'steals': 1,\n",
              " 'from': 1,\n",
              " 'under': 1,\n",
              " 'fishburne': 1,\n",
              " 'nose': 1,\n",
              " 'and': 1,\n",
              " 'there': 1,\n",
              " 'talented': 1,\n",
              " 'cast': 1,\n",
              " 'good': 1,\n",
              " 'form': 1}"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 単語ごとの数のリストの0番目を表示\n",
        "word_count_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8NZjYT0_aLX"
      },
      "source": [
        "### 行列への変換"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTWbRq4T_bNO"
      },
      "outputs": [],
      "source": [
        "# DictVectorizerを使用して行列に変換し、datasetに格納する\n",
        "vectorizer = DictVectorizer(sparse=True)\n",
        "dataset = vectorizer.fit_transform(word_count_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeeGumr4_fT5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset.shape= (50000, 101249)\n"
          ]
        }
      ],
      "source": [
        "# datasetの大きさを確認\n",
        "print(\"dataset.shape=\", dataset.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCvgn5UW_i2Y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(feature_names)= 101249\n",
            "最初の20個を表示 ['\\x08\\x08\\x08\\x08a' '\\x10own' '\\\\and\\\\' '`accident' '`action' '`actors'\n",
            " '`addicted' '`adelaide' '`adolf' '`adolph' '`adventure' '`afterlife'\n",
            " '`agent' '`air' '`alfred' '`alien' '`alive' '`all' '`alone' '`america']\n"
          ]
        }
      ],
      "source": [
        "# 各列に対応した単語を取得\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "print(\"len(feature_names)=\", len(feature_names))\n",
        "print(\"最初の20個を表示\", feature_names[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nc7Xp8D_xC2"
      },
      "source": [
        "## 4. 機械学習の実施"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoLhZFvz_mSo"
      },
      "outputs": [],
      "source": [
        "# 必要なライブラリの追加import（変更しないでください）\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f5_xO8_KDje"
      },
      "source": [
        "目的変数と説明変数を用意します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "le7zEl0BIxWr"
      },
      "outputs": [],
      "source": [
        "# 目的変数Yの用意\n",
        "# neg12500 + pos12500 + neg12500 + pos12500 = 50000\n",
        "Y = np.array(\n",
        "    [0] * len(train_neg_files) +\n",
        "    [1] * len(train_pos_files) +\n",
        "    [0] * len(test_neg_files) +\n",
        "    [1] * len(test_pos_files)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jsy8g6L2JHdo"
      },
      "outputs": [],
      "source": [
        "# 上記のY、および前処理されたdatasetからデータを分割し、\n",
        "# X_train, Y_train, X_test, Y_testに格納する\n",
        "#\n",
        "# 詳細：\n",
        "#   - dataset の先頭から25000件を 変数 X_train に、残りを変数 X_test に代入\n",
        "#   - 目的変数 Y の先頭から25000件を 変数 Y_train に、残りを Y_test に代入\n",
        "X_train = dataset[:25000]\n",
        "X_test  = dataset[25000:]\n",
        "Y_train = Y[:25000]\n",
        "Y_test  = Y[25000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z437TWo0Ky5w"
      },
      "outputs": [],
      "source": [
        "# X_trainとY_trainを、train_test_splitで7:3に分割し、3割のほうを検証データ（X_valid, Y_valid）にする\n",
        "X_train, X_valid, Y_train, Y_valid = train_test_split(\n",
        "    X_train, Y_train, test_size=0.3, random_state=RANDOM_STATE, stratify=Y_train\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c31t4guALPTO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RANDOM_STATE= 42\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg     0.8795    0.8781    0.8788      3750\n",
            "         pos     0.8783    0.8797    0.8790      3750\n",
            "\n",
            "    accuracy                         0.8789      7500\n",
            "   macro avg     0.8789    0.8789    0.8789      7500\n",
            "weighted avg     0.8789    0.8789    0.8789      7500\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ロジスティック回帰モデルを作成し、学習して、検証データによる予測を実施する\n",
        "logreg = LogisticRegression(solver=\"liblinear\", max_iter=1000, random_state=RANDOM_STATE)\n",
        "logreg.fit(X_train, Y_train)\n",
        "valid_pred = logreg.predict(X_valid)\n",
        "\n",
        "# classification_reportを実行し、検証データによるモデルの評価を行なう\n",
        "print(\"RANDOM_STATE=\",RANDOM_STATE)\n",
        "print(classification_report(Y_valid, valid_pred, target_names=[\"neg\", \"pos\"], digits=4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aiRUHbnM3Ac"
      },
      "source": [
        "## 5. テストデータによる評価\n",
        "\n",
        "最後に、テストデータで評価を行ないましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpHmZx8pM3V7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RANDOM_STATE= 42\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg     0.8610    0.8690    0.8650     12500\n",
            "         pos     0.8678    0.8597    0.8637     12500\n",
            "\n",
            "    accuracy                         0.8644     25000\n",
            "   macro avg     0.8644    0.8644    0.8644     25000\n",
            "weighted avg     0.8644    0.8644    0.8644     25000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# テストデータで予測を実施する\n",
        "test_pred = logreg.predict(X_test)\n",
        "\n",
        "# classification_reportを実行し、テストデータによるモデルの評価を行なう\n",
        "print(\"RANDOM_STATE=\",RANDOM_STATE)\n",
        "print(classification_report(Y_test, test_pred, target_names=[\"neg\", \"pos\"], digits=4))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
